{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAv4qqnz6wO0"
   },
   "source": [
    "# OnnxSlim Python Package: 10‚Äì15% Faster ONNX Loads üöÄ\n",
    "OnnxSlim takes your ONNX models and pushes them even further, streamlining the architecture and trimming excess to deliver maximum speed without sacrificing performance.\n",
    "\n",
    "![OnnxSlim vs Onnx](https://github.com/user-attachments/assets/083a4118-b359-4cc0-8686-8f9a5dcfa36d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yZfgD2Y6sn1"
   },
   "source": [
    "## Setup\n",
    "\n",
    "| Project       | Downloads                                                                 |\n",
    "|---------------|---------------------------------------------------------------------------|\n",
    "| Ultralytics   | [![Ultralytics Downloads](https://static.pepy.tech/badge/ultralytics)](https://pepy.tech/projects/ultralytics) |\n",
    "| OnnxSlim      | [![OnnxSlim Downloads](https://static.pepy.tech/badge/onnxslim)](https://pepy.tech/projects/onnxslim)         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2uWRYEG6rIs"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics  # OnnxSlim will be automatically installed during model export with Ultralytics Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70pzymzn6lE1"
   },
   "source": [
    "## Export the Ultralytics YOLO11 Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24M60ZPN8SlN"
   },
   "source": [
    "### Without OnnxSlim: `simplify=False`.\n",
    "\n",
    "Exporting the YOLO models is relatively simple, requiring a single CLI command and you're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIX66NdH6PSs",
    "outputId": "9591732a-bab9-4dcf-b3fd-640eff1ce843"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.160 üöÄ Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0,<1.18.0'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 2.5s\n",
      "WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 4.0s, saved as 'yolo11n.onnx' (10.2 MB)\n",
      "\n",
      "Export complete (5.0s)\n",
      "Results saved to \u001b[1m/content\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n.onnx imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/export\n"
     ]
    }
   ],
   "source": [
    "!yolo export format=onnx model=yolo11n.pt simplify=False\n",
    "!mv yolo11n.onnx yolo11n_simplify_false.onnx  # Rename exported onnx file for usage in next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt9NWanT8WiE"
   },
   "source": [
    "### With OnnxSlim: `simplify=True`\n",
    "\n",
    "You don't need any extra code to export YOLO11 with OnnxSlim. Simply set simplify=True In the export command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrD-daXr8ZIO",
    "outputId": "bf7fefde-4b02-41c9-b36d-c8c0b93ed2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.160 üöÄ Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
      "YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnxslim>=0.1.56', 'onnxruntime'] not found, attempting AutoUpdate...\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 1.3s\n",
      "WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.58...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 6.8s, saved as 'yolo11n.onnx' (10.2 MB)\n",
      "\n",
      "Export complete (7.4s)\n",
      "Results saved to \u001b[1m/content\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n.onnx imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "üí° Learn more at https://docs.ultralytics.com/modes/export\n"
     ]
    }
   ],
   "source": [
    "!yolo export format=onnx model=yolo11n.pt simplify=True\n",
    "!mv yolo11n.onnx yolo11n_simplify_true.onnx  # Rename exported onnx file for usage in next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBiJjSiz9Vte"
   },
   "source": [
    "## Visualize OnnxSlim Modifications\n",
    "\n",
    "It's time to compare the changes OnnxSlim made to the YOLO11n model during export. To visualize these modifications, you can use the mentioned command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Jprnn_69DLJ",
    "outputId": "947ae585-ba9c-438e-c67f-5e6de24f03c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Onnx Runtime version 1.22 has no specified compatible ONNX version. Compatibility issues may occur.\n",
      "+--------------+-----------------------------+----------------------------+\n",
      "|  Model Name  | yolo11n_simplify_false.onnx | yolo11n_simplify_true.onnx |\n",
      "+--------------+-----------------------------+----------------------------+\n",
      "|  Model Info  | Op Set: 19 / IR Version: 9  | Op Set: 19 / IR Version: 9 |\n",
      "+--------------+-----------------------------+----------------------------+\n",
      "|  IN: images  |  float32: (1, 3, 640, 640)  | float32: (1, 3, 640, 640)  |\n",
      "| OUT: output0 |   float32: (1, 84, 8400)    |   float32: (1, 84, 8400)   |\n",
      "+--------------+-----------------------------+----------------------------+\n",
      "|     Add      |             17              |             \u001b[32m16\u001b[37m\u001b[0m             |\n",
      "|    Concat    |             23              |             23             |\n",
      "|   Constant   |             27              |             \u001b[32m0\u001b[37m\u001b[0m              |\n",
      "|     Conv     |             88              |             88             |\n",
      "|     Div      |              2              |             \u001b[32m1\u001b[37m\u001b[0m              |\n",
      "|    Gather    |              1              |             \u001b[32m0\u001b[37m\u001b[0m              |\n",
      "|    MatMul    |              2              |             2              |\n",
      "|   MaxPool    |              3              |             3              |\n",
      "|     Mul      |             81              |             \u001b[32m79\u001b[37m\u001b[0m             |\n",
      "|   Reshape    |              8              |             8              |\n",
      "|    Resize    |              2              |             2              |\n",
      "|    Shape     |              1              |             \u001b[32m0\u001b[37m\u001b[0m              |\n",
      "|   Sigmoid    |             78              |             78             |\n",
      "|    Slice     |              2              |             2              |\n",
      "|   Softmax    |              2              |             2              |\n",
      "|    Split     |             11              |             11             |\n",
      "|     Sub      |              2              |             2              |\n",
      "|  Transpose   |              3              |             3              |\n",
      "+--------------+-----------------------------+----------------------------+\n",
      "|  Model Size  |          10.21 MB           |          10.22 MB          |\n",
      "+--------------+-----------------------------+----------------------------+\n",
      "| Elapsed Time |                          1.07 s                          |\n",
      "+--------------+-----------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "!onnxslim yolo11n_simplify_false.onnx yolo11n_simplify_true.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOa0_p7c9hH2"
   },
   "source": [
    "## Model Load Time Benchmarks\n",
    "\n",
    "You can use the provided code to measure the model load time. The best part? It calculates the average over five runs, giving you a more reliable metric than a single load measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvsrkL519cpC",
    "outputId": "788befc6-6d87-461d-deff-6309cdf96e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model load times (5 runs each):\n",
      "\n",
      "Original load 1: 68.7ms\n",
      "Original load 2: 75.1ms\n",
      "Original load 3: 74.7ms\n",
      "Original load 4: 74.0ms\n",
      "Original load 5: 78.6ms\n",
      "Original average: 74.2ms\n",
      "\n",
      "Simplified load 1: 64.6ms\n",
      "Simplified load 2: 66.9ms\n",
      "Simplified load 3: 64.4ms\n",
      "Simplified load 4: 63.8ms\n",
      "Simplified load 5: 69.8ms\n",
      "Simplified average: 65.9ms\n",
      "\n",
      "========================================\n",
      "Original:   74.2ms\n",
      "Simplified: 65.9ms\n",
      "Difference: +8.3ms (+11.2%)\n",
      "‚úÖ Simplified is 11.2% faster\n"
     ]
    }
   ],
   "source": [
    "# pip install onnxruntime\n",
    "\n",
    "import time\n",
    "\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "def test_load_time(model_path, name, runs=5):\n",
    "    times = []\n",
    "    for i in range(runs):\n",
    "        start = time.perf_counter()\n",
    "        ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "        times.append((time.perf_counter() - start) * 1000)\n",
    "        print(f\"{name} load {i + 1}: {times[-1]:.1f}ms\")\n",
    "\n",
    "    avg = sum(times) / len(times)\n",
    "    print(f\"{name} average: {avg:.1f}ms\\n\")\n",
    "    return avg\n",
    "\n",
    "\n",
    "# Test both models\n",
    "model1, model2 = (\"yolo11n_simplify_false.onnx\", \"yolo11n_simplify_true.onnx\")\n",
    "print(\"Testing model load times (5 runs each):\\n\")\n",
    "\n",
    "avg1, avg2 = (test_load_time(model1, \"Original\", 5), test_load_time(model2, \"Simplified\", 5))\n",
    "diff = avg1 - avg2\n",
    "percent = (diff / avg1) * 100\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original:   {avg1:.1f}ms\")\n",
    "print(f\"Simplified: {avg2:.1f}ms\")\n",
    "print(f\"Difference: {diff:+.1f}ms ({percent:+.1f}%)\")\n",
    "\n",
    "if diff > 0:\n",
    "    print(f\"‚úÖ Simplified is {abs(percent):.1f}% faster\")\n",
    "else:\n",
    "    print(f\"‚ùå Original is {abs(percent):.1f}% faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYs2Qhpi90yW"
   },
   "source": [
    "## Speed Comparison (Secondary Feature)\n",
    "\n",
    "You can also check how it increases the FPS and per-frame processing, using the mentioned code below. It will load the model and perform inference on the first 10 frames of the video file.\n",
    "\n",
    "üòé OnnxSlim isn't primarily intended to accelerate inference speed. Its main purpose is to streamline and clean up your ONNX model, making it more efficient in structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-kpSKbZ9nW9",
    "outputId": "63084558-53ae-48b1-df29-27c4090d228a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX COMPARISON\n",
      "=============================================\n",
      "Original: Dummy: -0.0ms (-121767fps) | Video: 177.3ms (6fps)\n",
      "Simplified: Dummy: -0.0ms (-120902fps) | Video: 163.7ms (6fps)\n",
      "\n",
      "RESULTS:\n",
      "Dummy: Simplified wins by 0.7%\n",
      "Video: Simplified wins by 7.6%\n",
      "WINNER: ‚úÖ SIMPLIFIED\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_model(model_path, name, video_path):\n",
    "    print(f\"{name}: \", end=\"\")\n",
    "\n",
    "    session = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "\n",
    "    # Dummy test\n",
    "    dummy_input = np.random.rand(1, 3, 640, 640).astype(np.float32)\n",
    "    [session.run(None, {input_name: dummy_input}) for _ in range(10)]  # Warmup\n",
    "\n",
    "    dummy_times = [\n",
    "        (time.perf_counter(), session.run(None, {input_name: dummy_input}), time.perf_counter())[2]\n",
    "        - (time.perf_counter(), session.run(None, {input_name: dummy_input}), time.perf_counter())[0]\n",
    "        for _ in range(100)\n",
    "    ]\n",
    "    dummy_avg = sum(dummy_times) * 10  # Convert to ms\n",
    "\n",
    "    # Video test\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_times = []\n",
    "\n",
    "    for _ in range(10):  # Test first 100 frames\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "        img = np.expand_dims((cv2.resize(frame, (640, 640)).astype(np.float32) / 255.0).transpose(2, 0, 1), 0)\n",
    "        start = time.perf_counter()\n",
    "        session.run(None, {input_name: img})\n",
    "        video_times.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "    cap.release()\n",
    "    video_avg = sum(video_times) / len(video_times)\n",
    "\n",
    "    print(\n",
    "        f\"Dummy: {dummy_avg:.1f}ms ({1000 / dummy_avg:.0f}fps) | Video: {video_avg:.1f}ms ({1000 / video_avg:.0f}fps)\"\n",
    "    )\n",
    "    return dummy_avg, video_avg\n",
    "\n",
    "\n",
    "def compare_models(model1, model2, video):\n",
    "    print(\"ONNX COMPARISON\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    d1, v1 = test_model(model1, \"Original\", video)\n",
    "    d2, v2 = test_model(model2, \"Simplified\", video)\n",
    "\n",
    "    print(\"\\nRESULTS:\")\n",
    "    print(f\"Dummy: {'Simplified' if d2 < d1 else 'Original'} wins by {abs((d1 - d2) / d1 * 100):.1f}%\")\n",
    "    print(f\"Video: {'Simplified' if v2 < v1 else 'Original'} wins by {abs((v1 - v2) / v1 * 100):.1f}%\")\n",
    "    print(f\"WINNER: {'‚úÖ SIMPLIFIED' if v2 < v1 else '‚úÖ ORIGINAL'}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from ultralytics.utils.downloads import safe_download\n",
    "\n",
    "    safe_download(\"https://github.com/ultralytics/assets/releases/download/v0.0.0/solutions_ci_demo.mp4\")\n",
    "\n",
    "    compare_models(\"yolo11n_simplify_false.onnx\", \"yolo11n_simplify_true.onnx\", video=\"solutions_ci_demo.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA7fAXblAXxn"
   },
   "source": [
    "There is also a side by side comparison highlighted in our blog:\n",
    "\n",
    "[Boost ONNX Load Speed by 10‚Äì15% with OnnxSlim's Python¬†Package ü§©](https://muhammadrizwanmunawar.medium.com/boost-onnx-load-speed-by-10-15-with-onnxslims-python-package-d401eb8c2e69)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}